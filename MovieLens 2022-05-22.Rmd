---
title: "MovieLens"
author: "Matej Sebenik"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Data download and processing, as provided by the course, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide',fig.keep='all'}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
library(stringi)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
#movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#                                           title = as.character(title),
#                                           genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```

```{r Further data manipulation, with the goal of obtaining better results, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide',fig.keep='all'}
# Install package for pdf creation
tinytex::install_tinytex()

# Force the values displayed in the graphs NOT to be in the scientific format (e.g. 1.3e08)
options(scipen=999)

# backup the dataset provided by the course instructors.
#---------------------------------------------------------------------------------------------------
edx_original = edx
validation_original = validation

# Transform the data provided, by including columns regarding genres, time dimensions etc.
#---------------------------------------------------------------------------------------------------
# The lines of code below handle rows that only have one genre assigned to it.
# Create a column named single_genre, which contains 1 if the genres column contains only
# one genre, and 0 if it contains more than one genre.
edx = edx %>% mutate("single_genre" = ifelse(edx$genres %in% c("Comedy","Drama","Thriller","Western","Horror","Documentary","Action","Romance",
                  "Sci-Fi","Children","Adventure","Animation","Musical","Film-Noir","Crime","War",
                  "Mystery","Fantasy","IMAX","(no genres listed)") ==TRUE,1,0))

validation = validation %>% mutate("single_genre" = ifelse(validation$genres %in% c("Comedy","Drama","Thriller","Western","Horror","Documentary","Action","Romance",
                                                               "Sci-Fi","Children","Adventure","Animation","Musical","Film-Noir","Crime","War",
                                                               "Mystery","Fantasy","IMAX","(no genres listed)") ==TRUE,1,0))

# If the movie has only one genre, retain the genre, otherwise return NA.
edx = edx %>% mutate("single_genre_title" = ifelse(single_genre == 1, genres, NA))

validation = validation %>% mutate("single_genre_title" = ifelse(single_genre == 1, genres, NA))

#---------------------------------------------------------------------------------------------------
# The following lines recode the genres column to numbers, since calculating anything with original
# genres column causes immense problems due to the size of the column - it can contain a lot of
# letters.
# Select the column genres, then obtain its distinct values.
edx_genres = edx %>% select(genres)
distinct_edx_genres = distinct(edx_genres)

validation_genres = validation %>% select(genres)
distinct_validation_genres = distinct(validation_genres)

# Assign each of the distinct values a number.
set.seed(1, sample.kind = "Rounding")
distinct_edx_genres_numbers = data.frame(distinct_edx_genres, genre_number = sample(seq(1, nrow(distinct_edx_genres), 1), nrow(distinct_edx_genres), replace = FALSE))
distinct_edx_genres_numbers %>% arrange(genre_number)

set.seed(1, sample.kind = "Rounding")
distinct_validation_genres_numbers = data.frame(distinct_validation_genres, genre_number = sample(seq(1, nrow(distinct_validation_genres), 1), nrow(distinct_validation_genres), replace = FALSE))
distinct_validation_genres_numbers %>% arrange(genre_number)

# Merge the number of each genre back into the edx dataset. Remove the name of genre.
edx_gennum = merge(edx, distinct_edx_genres_numbers)
edx_gennum_nogenre = edx_gennum %>% select(-genres)

validation_gennum = merge(validation, distinct_validation_genres_numbers)
validation_gennum_nogenre = validation_gennum %>% select(-genres)

#---------------------------------------------------------------------------------------------------
# The following line extracts the year of movie creation from the movie name.
edx_new = edx_gennum_nogenre %>% 
  mutate(movie_year = as.numeric(substr(stri_sub(edx_gennum_nogenre$title,-6),2,5)))

validation_new = validation_gennum_nogenre %>% 
  mutate(movie_year = as.numeric(substr(stri_sub(validation_gennum_nogenre$title,-6),2,5)))

#---------------------------------------------------------------------------------------------------
# The following lines calculates the number of entries, mean and standard deviation of rating for each genre.
edx_new_genre = edx_new %>% group_by(genre_number) %>%
  summarise(n_genre = n(), mean_genre = mean(rating), sd_genre = sd(rating))
edx_new_genre_rejoined = merge(edx_new, edx_new_genre,by = "genre_number")
edx_new = edx_new_genre_rejoined

validation_new_genre = validation_new %>% group_by(genre_number) %>%
  summarise(n_genre = n(), mean_genre = mean(rating), sd_genre = sd(rating))
validation_new_genre_rejoined = merge(validation_new, validation_new_genre,by = "genre_number")
validation_new = validation_new_genre_rejoined

#---------------------------------------------------------------------------------------------------
# The following lines calculate the number of entries, mean and standard deviation of rating for each movie.
edx_new_movie = edx_new %>% group_by(movieId) %>%
  summarise(n_movie = n(), mean_movie = mean(rating), sd_movie = sd(rating))
edx_new_movie_rejoined = merge(edx_new, edx_new_movie,by = "movieId")

validation_new_movie = validation_new %>% group_by(movieId) %>%
  summarise(n_movie = n(), mean_movie = mean(rating), sd_movie = sd(rating))
validation_new_movie_rejoined = merge(validation_new, validation_new_movie,by = "movieId")

#---------------------------------------------------------------------------------------------------
# The following line calculates the year of rating of the movie. The timestamp column contains 
# the seconds after 01.01.1970 the rating took place (https://files.grouplens.org/datasets/movielens/ml-10m-README.html).
# Thus, the time stamp value is transformed to minutes, hours, days, and (taking account of leap years)
# to years. Finally, the title of the movie is removed.
edx_new = edx_new_movie_rejoined %>% mutate(year_timestamp = round(timestamp/(60*60*24*((365*3+366)/4))+1970)) %>% 
  select(-title)

validation_new = validation_new_movie_rejoined %>% mutate(year_timestamp = round(timestamp/(60*60*24*((365*3+366)/4))+1970)) %>% 
  select(-title)
#---------------------------------------------------------------------------------------------------
# Delete the redundant stored objects, and clear the computer memory.
rm(edx_gennum, edx_gennum_nogenre, edx_new_genre, edx_new_genre_rejoined, edx_new_movie, 
   edx_new_movie_rejoined, distinct_edx_genres, edx_genres)
rm(validation_gennum, validation_gennum_nogenre, validation_new_genre, validation_new_genre_rejoined, validation_new_movie, 
   validation_new_movie_rejoined, distinct_validation_genres, validation_genres)
gc()
```

```{r RMSE calculation, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, results='hide',fig.keep='all'}
# Calculate the RMSE. 
#---------------------------------------------------------------------------------------------------
# The method returning the best (lowest) RMSE was loess. RMSE improved with the
# reduction of span - the value of 0.15 was chosen, as lower values could lead to overfitting.
# Similarly the degree of 1 was used to improve the RMSE. Cross valudation was used both to
# improve the RMSE and to speed up the calculation times, with the value of 7 chosen.
# Timestamp was substituted by the year extracted from it, and the mean value of a movie was
# not used to avoid overtraining. Standard deviations did not present a meaningful improvement
# to RMSE and were dropped.

# Fair warning - the calculation of the algorithm took 15 minutes on 64 GB of ddr5 ram and i9-12900k
# Intel processor. The average memory usage was about 21 GB. Which shows that good computers are
# no substitute for statistics and coding knowledge. Run the code below at your own discretion.
gc()
edx_control_loess <- trainControl(method = "cv", number = 7)
#grid <- expand.grid(span = 0.35, degree = 1)
set.seed(1, sample.kind = "Rounding")
edx_loess <- train(rating ~ 
                  genre_number
                  +userId
                  +movieId
                        #+timestamp #+mean_movie
                  +year_timestamp
                  +n_genre
                  +mean_genre
                  #+sd_genre
                  +single_genre
                  +movie_year
                  +n_movie
                  #+sd_movie
                  , method = "gamLoess",
                  #tuneGrid=grid,
                  data = edx_new
                  ,trControl = edx_control_loess
)
edx_loess
RMSE_initial = round(edx_loess$results["RMSE"], 5)

# Run the algorithm on the validation data set to obtain the final RMSE.
validation_prediction = predict(edx_loess, validation_new)

RMSE_final = round(RMSE(validation_prediction, validation_new$rating), 5)
```

***1. Introduction:***  
The goal of this paper is the creation of a predictive algorithm, based on the "movielens" data set, that will predict the rating of movies. The metrics used to evaluate the algorithm is the RMSE (root mean square error). 

The data set is available at [**this link**](https://files.grouplens.org/datasets/movielens/ml-10m.zip).

The data set is previewed below:

```{r head of edx_original, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, fig.keep='all'}
head(edx_original)
```

The data set is divided into 6 columns. These contain:  
1. The Id of the user (numeric).  
2. The Id the movie (numeric).  
3. The rating (numeric, from 0.5 to 5, with 10 possible ratings (0.5, 1, 1.5,..., 4.5, 5)).  
4. The timestamp (numeric, the time of the rating creation, expressed in seconds elapsed since 01.01.1970).  
5. The title (character, also contains the year the movie was made).  
6. Genres (character, can be a combination of many individual genres).  

The data set has `r nrow(edx_original)+nrow(validation_original)` entries/rows.

Key steps in the calculation of the predictive algorithm are as follows:  
1. Data set download.  
2. Initial data preparation and data cleaning.  
3. Additional data modification.  
4. Data visualizations.  
5. Algorithm training.  
6. Algorithm verification.  


***2. Methods, analysis, data preparation and visualizations:***  
**2.1. Methods, analysis, data preparation**  
For the data set download, as well as initial data preparation and data cleaning, the R code was (thoughtfully) provided by the edx/Harvard course instructors/assistants. The compressed data was downloaded, unzipped, and the columns named. Additionally, the data set was split into the edx (training/test) set (90% of the original data) and validation set (only to be used for algorithm verification) (10% of the original data).

Through work on the algorithm, additional steps were taken with the goal of improving (lowering) the RMSE. These steps were applied equally to the edx and validation data sets described above. These steps (broadly) included:  
1. Determining if an entry had only one genre assigned to it (e.g. "Action") as opposed to multiple genres (e.g. "Action|Adventure"). Single genres might get higher or lower ratings than multiple genres.  
2. Genres are presented as words. This makes them unnecessarily long and unwieldy for use. Regressions using the original genres column would usually not run due to memory limitations. As such, the genres column was recoded to numeric values, which avoid the pitfalls listed above.  
3. As stated above, each title of the movie also contains the year it was filmed/created. This value was extracted into a separate column, as movies created in different years might get different ratings (alluding to a "Golden Age" of movie making, when movies were simply better than at other times).  
4. For each genre, the number of ratings, mean of ratings and standard deviation of ratings was calculated. Certain genres might be more popular than other, getting more/higher ratings than others.  
5. Similarly, the number of ratings, mean of ratings and standard deviation was calculated for individual movies, at the risk of increased overtraining.  
6. Lastly the year of rating was extracted from the timestamp column, roughly following the same logic as the year of movie creation mentioned under the third point (above).  

After further data preparation, the final data set used looks like this:

```{r head of edx_new, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, fig.keep='all'}
head(edx_new)
```


**2.2. Data visualizations:**  
Several data visualizations were considered and prepared, with the aim of getting acquainted with the data. These visualizations are provieded below, along with the relevant descriptions and insights.

1. Histogram of ratings:

```{r Histogram of ratings, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, fig.keep='all'}
edx %>% ggplot(aes(rating)) + geom_histogram() + ggtitle("Frequency of ratings") +
  xlab("Rating") + ylab("No. of ratings")
```

This histogram shows the number of ratings per each rating level, useful for general rating distribution overview.
Two findings stand out; the "whole number" ratings (e.g. 1, 3) are much more likely to be chosen than those with decimal points (e.g. 1.5, 3.5). The distribution also does not seem to be entirely normal; for that the peak of ratings should be at rating value 3, not 4 (the rating curve is skewed to the right).

2. Number of ratings and mean rating per year of rating:

```{r Combo line chart, no. of ratings and mean rating, per year of rating, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, fig.keep='all'}
# Mean rating vs. no. of ratings according to the year of rating (from timestamp).
# Calculate the ratio of the mean rating vs. number of ratings. This will enable the proper
# combo graph to be displayed, by correcting the scale of the second y axis.
year_timestamp_data = edx_new %>% group_by(year_timestamp) %>%
  summarise(mean = mean(rating), n = n())
year_timestamp_data_coeff = max(year_timestamp_data$mean)/max(year_timestamp_data$n)

# The code below draws the combo graph, which has different values on each y axis. The axis and
# lines are colored the same color, so that we can recognize which is which.
edx_new %>% group_by(year_timestamp) %>%
  summarise(mean = mean(rating), n = n()) %>%
  ggplot(aes(x = year_timestamp, group = 1)) +
  geom_line(aes(y = mean), col="blue") +
  geom_line(aes(y = n*year_timestamp_data_coeff), col="red") +
  scale_y_continuous(name = "Mean rating", sec.axis = sec_axis(~./year_timestamp_data_coeff, name="No. of ratings")) +
  ggtitle("Mean rating vs. no. of ratings per year of rating") +
  xlab("Year of rating") +
  theme(axis.title.y.left=element_text(color="blue"),
        axis.text.y.left=element_text(color="blue"),
        axis.title.y.right=element_text(color="red"),
        axis.text.y.right=element_text(color="red"))
```

This graph shows the number of ratings and mean rating per year of rating.
Curiously, it shows that the number of ratings per year of rating is highly irregular, swinging wildly without any obvious pattern. The mean rating, in contrast, remains fairly constant (aside from an initial high point, which corresponds to extremely low number of ratings). There is also no obvious connection between the no. of ratings and mean of ratings - sometimes higher number of ratings corresponds to higher ratings (year 2000), sometimes to lower ratings (year 2005), and sometimes there is no visible impact (year 1997).

3. Number of ratings per year of rating and individual rating:

```{r Line chart, no. of ratings per year of rating and individual rating, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, fig.keep='all'}
# No. of ratings per year of rating, divided by individual rating.
edx_new %>% mutate(rating = as.character(rating)) %>%
  ggplot(aes(x = year_timestamp, group = rating, color = rating), label = rating) +
  geom_line(stat = 'count') + ggtitle("Frequency of ratings per rating and year of rating") +
  xlab("Year of rating") + ylab("No. of ratings")
```

This graph shows the number of ratings per year of rating, divided into lines for individual ratings.
Interestingly, this graph shows that the "decimal point ratings" (e.g. 2.5) only came into use in 2003. The year 2004 was the only year when the "decimal point ratings" and "whole number ratings" did not move in concert. Otherwise the individual rating numbers all follow the same dynamic of rises/falls in frequency of ratings.

4. Number of ratings and mean rating per year of movie creation:

```{r Combo line chart, no. of ratings and mean rating per year of movie creation, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, fig.keep='all'}
# Mean rating vs. no. of ratings according to the year of movie creation.
# Calculate the ratio of the mean rating vs. number of ratings. This will enable the proper
# combo graph to be displayed, by correcting the scale of the second y axis.
year_created_data = edx_new %>% group_by(movie_year) %>%
  summarise(mean = mean(rating), n = n())
year_created_data_coeff = max(year_created_data$mean)/max(year_created_data$n)

# The code below draws the combo graph, which has different values on each y axis. The axis and
# lines are colored the same color, so that we can recognize which is which.
edx_new %>% group_by(movie_year) %>%
  summarise(mean = mean(rating), n = n()) %>%
  ggplot(aes(x = movie_year, group = 1)) +
  geom_line(aes(y = mean), col="blue") +
  geom_line(aes(y = n*year_created_data_coeff), col="red") +
  scale_y_continuous(name = "Mean rating", sec.axis = sec_axis(~./year_created_data_coeff, name="No. of ratings")) +
  ggtitle("Mean rating vs. no. of ratings per year of movie creation") +
  xlab("Year of movie creation") +
  theme(axis.title.y.left=element_text(color="blue"),
        axis.text.y.left=element_text(color="blue"),
        axis.title.y.right=element_text(color="red"),
        axis.text.y.right=element_text(color="red"))
```

This graph shows the number of ratings and mean rating per year of movie creation.
This graph shows a connection between the year of movie creation and the number and mean of associated ratings. Up to cca. year 1970 of movie creation, the number of ratings per year was relatively low, and the mean rating was relatively high. After 1970, the dynamic changes, with a huge spike of ratings for newer movies and corresponding lower mean ratings. Speculatively speaking, users might have rated their favorite old movies, giving them higher ratings. They also rated current (possibly trending) movies, finding them less appealing. This graph might provide some support for the "Golden Age" theory described above (second chapter, third line).

5. Number of ratings and mean rating per genre name (single genres only):

```{r Combo line chart, no. of ratings and mean rating per single genre name, echo=FALSE,warning=FALSE,message=FALSE,error=FALSE, fig.keep='all'}
# Mean rating vs. no. of ratings according to the genre, where only one genre per movie is specified.
# Calculate the ratio of the mean rating vs. number of ratings. This will enable the proper
# combo graph to be displayed, by correcting the scale of the second y axis.
genre_rating_data = edx_new %>% filter(single_genre == 1) %>% group_by(single_genre_title) %>%
  summarise(mean = mean(rating), n = n())
genre_rating_data_coeff = max(genre_rating_data$mean)/max(genre_rating_data$n)

# The code below draws the combo graph, which has different values on each y axis. The axis and
# lines are colored the same color, so that we can recognize which is which.
edx_new %>% filter(single_genre == 1) %>% group_by(single_genre_title) %>%
  summarise(mean = mean(rating), n = n()) %>%
  ggplot(aes(x = reorder(single_genre_title, mean), group = 1)) +
  theme(axis.text.x = element_text(angle = 65, hjust = 1)) +
  geom_line(aes(y = mean), col="blue") +
  geom_line(aes(y = n*genre_rating_data_coeff), col="red") +
  scale_y_continuous(name = "Mean rating", sec.axis = sec_axis(~./genre_rating_data_coeff, name="No. of ratings")) +
  ggtitle("Mean rating vs. no. of ratings per genre (single genres only)") +
  xlab("Genre name (single genres only)") +
  theme(axis.title.y.left=element_text(color="blue"),
        axis.text.y.left=element_text(color="blue"),
        axis.title.y.right=element_text(color="red"),
        axis.text.y.right=element_text(color="red"))
```

This graph shows the number of ratings and mean rating per genre name (only for movies which have only one genre assigned). Sorted by the mean rating.

The graph shows little association of no. of ratings with ratings per genre name. Comedy has a large number of ratings but average mean ratings, while Drama also has high number of ratings and a high mean rating.

***3. Modelling approach and results:***  
**3.1. Modelling approach:**  
The goal of the algorithm is to predict the rating of movies. As noted above, ratings fall into 10 distinct "categories", e.g. 0.5, 1, 1.5 etc. Crucially, users cannot rate an individual movie outside these ten categories, e.g. 1.73. This makes the rating a categorical type of variable, and not a continuous variable.

The distinction mentioned above is relevant since each type of variable has specific machine learning approaches associated with them. 
Thus, categorical types of variables generally imply the use of classification methods, such as LDA, QDA, decision trees and random forests. Crucially, they also imply **accuracy** as the metric used to evaluate the performance of the algorithm.
On the other hand, continuous variables are associated with various regressions and use the RMSE for performance evaluation.

According to the facts above, the correct metric to evaluate our model should be accuracy, but is actually RMSE. As the staff/assistants of the course helpfuly point out, RMSE actually has benefits as opposed to accuracy, in that if we predict the value of 3.99 for an actual number of 4, the accuracy metric would still report 0 accuracy (as the numbers are not identical). This accuracy value of 0 would be equal for values of 3.99 and 0.5, even though the value 3.99 is much closer to 4 than 0.5. The RMSE, on the other hand, would return a more favorable assessment of value 3.99 as opposed to the value 0.5.

Accordingly, and in line with the demands of the course, RMSE was used for evaluation and associated regressions were used to build the algorithm. Having insufficient time and knowledge to test all the regression methods available in the caret package (there are over a hundred), the most popular were tested, these being the linear regression, general linear model, localized regression and the k nearest neighbor method. Of these, localized regression (loess) slightly outperformed the linear regression and the general linear model. K nearest neighbor performed worst, taking huge amounts of time to complete and often returning errors or empty results (no doubt due to the lack of knowledge on the part of the data scientist involved). Thus, localized regression method was chosen for the finalization of the algorithm.

The data set was not further divided into training and test sets. Instead, cross validation was used for initial model evaluation. The reason for this was two fold; firstly, cross validation returned slightly better RMSE values. Secondly and more importantly, it was also significantly faster than conventional approach with training and test sets. Spans and degrees were not regulated. Lowering the span toward 0 and setting the degree to 1 improved RMSE marginally, but occasionally caused errors during subsequent work. Constant reductions of the span also fueled concerns of overtraining.

As for the data used in the algorithm, the following were used:  
1. genre_number (recoded name of genre),  
2. userId,  
3. movieId (recoded name of movie),  
4. year_timestamp (year of rating),  
5. n_genre (number of ratings for the genre),  
6. mean_genre (mean rating for the genre),  
7. single_genre (1 if the movie only has one genre associated with it),  
8. movie_year (the year of the movie creation),  
9. n_movie (how many times the movie was rated).  

Several other variables were considered and discarded, such as:  
1. timestamp (unnecessary, since the year_timestamp adequately replaced it),  
2. mean_movie (mean rating of a specific movie. A huge contributor of overtraining, especially where the number of ratings for the given movie was low),  
3. sd_genre (standard deviation of the ratings in associated genre, discarded since the improvement to the RMSE using it was marginal at best),  
4. sd_movie (standard deviation of the movie, discarded since the improvement to the RMSE using it was marginal at best).  

**3.2. Results:**
The initial RMSE, calculated on the training/test data set, was a disappointing **`r RMSE_initial`**, which showed only marginal improvement over the most basic linear models and was far off of the range of RMSE the course instructors/staff considered necessary to obtain better grades. Unfortunately, the final RMSE, calculated by predicting the rating of the validation data set with the developed algorithm, revealed an even worse result at ***`r RMSE_final`***. This result is in line with the most basic linear models mentioned above. That, in turn, raises a very relevant question of either severe mistakes in the algorithm development or a case of overtraining.

Additionally, the time and resources it takes to develop the algorithm leave much to be desired. The runtime of the final model training takes about 15 minutes, on a very high end personal computer. The calculation itself is also memory demanding, at cca. 20 GB of RAM, though the prediction itself on the validation (final) data set was more tolerable.


***4. Conclusion:***
This report presented the development and results of the algorithm used to predict the ratings of movies in the MovieLens data set. Irrespective of the time and effort applied, its limitations are obvious in its poor RMSE results and poor training times. In retrospect, certain improvements could be made, such as substituting mean ratings for median ratings etc., but it is this data scientist nagging suspicion that the entire training algorithm design is deeply flawed in some fundamental way and cannot be easily improved upon.

As such, if this project should ever be revisited, the best course of action might be to simply start over from scratch.

Best regard,

Matej Sebenik  
Ljubljana  
Slovenia  
EU  